#+TITLE: Getting Started with pytest

* Run Tests

#+BEGIN_SRC python :tangle test_one.py
def test_passing():
    assert (1, 2, 3) == (1, 2, 3)
#+END_SRC

#+BEGIN_SRC bash :results verbatim
pytest -v test_one.py
#+END_SRC

#+begin_example
============================= test session starts ==============================
platform linux -- Python 3.8.6, pytest-6.2.3, py-1.10.0, pluggy-0.13.1 -- /home/michael/mainvenv/bin/python3
cachedir: .pytest_cache
rootdir: /home/michael/Documents/org_files/pytest
plugins: Faker-8.1.0, typeguard-2.11.1
collecting ... collected 1 item

test_one.py::test_passing PASSED                                         [100%]

============================== 1 passed in 0.03s ===============================
#+end_example

#+BEGIN_SRC python :tangle test_two.py
def test_failing():
    assert (1, 2, 3) == (3, 2, 1)
#+END_SRC

#+BEGIN_SRC bash :results raw
pytest -v test_two.py
#+END_SRC

#+begin_example
======================== test session starts ========================
platform linux -- Python 3.8.6, pytest-6.2.3, py-1.10.0, pluggy-0.13.1 -- /home/michael/mainvenv/bin/python3
cachedir: .pytest_cache
rootdir: /home/michael/Documents/org_files/pytest
plugins: Faker-8.1.0, typeguard-2.11.1
collected 1 item

test_two.py::test_failing FAILED                              [100%]

============================= FAILURES ==============================
___________________________ test_failing ____________________________

    def test_failing():
>       assert (1, 2, 3) == (3, 2, 1)
E       assert (1, 2, 3) == (3, 2, 1)
E         At index 0 diff: 1 != 3
E         Full diff:
E         - (3, 2, 1)
E         ?  ^     ^
E         + (1, 2, 3)
E         ?  ^     ^

test_two.py:2: AssertionError
====================== short test summary info ======================
FAILED test_two.py::test_failing - assert (1, 2, 3) == (3, 2, 1)
========================= 1 failed in 0.04s =========================
#+end_example

#+BEGIN_SRC python
from collections import namedtuple

Task = namedtuple("Task", ["summary", "owner", "done", "id"])

print(Task("yes", "owned", "nope", "10").owner)
#+END_SRC

#+BEGIN_SRC python :tangle tasks/test_three.py
# test the Task datatype
from collections import namedtuple

Task = namedtuple("Task", ["summary", "owner", "done", "id"])
Task.__new__.__defaults__ = (None, None, False, None)

def test_defaults():
    t1 = Task()
    t2 = Task(None, None, False, None)
    assert t1 == t2

def test_member_access():
    """
    Check field functionality of namedtuple
    """
    t = Task("buy oat milk", "brian")
    assert t.summary == "buy oat milk"
    assert t.owner == "brian"
    assert (t.done, t.id) == (False, None)
#+END_SRC

#+BEGIN_SRC python :tangle tasks/test_four.py
# test the Task datatype
from collections import namedtuple
Task = namedtuple("Task", ["summary", "owner", "done", "id"])
Task.__new__.__defaults__ = (None, None, False, None)

def test_asdict():
    """
    _asdict() should return a dictionary
    """
    t_task = Task("do something", "okken", True, 21)
    t_dict = t_task._asdict()
    expected = {"summary": "do something",
                "owner": "okken",
                "done": True,
                "id": 21}
    assert t_dict == expected

def test_replace():
    """
    replace() should change passed in fields
    """
    t_before = Task("finish book", "brian", False)
    t_after = t_before._replace(id=10, done=True)
    t_expected = Task("finish book", "brian", True, 10)
    assert t_after == t_expected
#+END_SRC

#+BEGIN_SRC bash
# given a whole directory
cd tasks
pytest

# specifying a directory
pytest tasks

# given a set of filenames
pytest test_three.py test_four.py
#+END_SRC

* Run a Single Test

#+BEGIN_SRC bash :dir ~/Documents/pytest :results verbatim
pytest -v tasks/test_four.py::test_asdict
#+END_SRC

#+begin_example
============================= test session starts ==============================
platform linux -- Python 3.8.6, pytest-6.2.3, py-1.10.0, pluggy-0.13.1 -- /home/michael/mainvenv/bin/python3
cachedir: .pytest_cache
rootdir: /home/michael/Documents/pytest
plugins: Faker-8.1.0, typeguard-2.11.1
collecting ... collected 1 item

tasks/test_four.py::test_asdict PASSED                                   [100%]

============================== 1 passed in 0.03s ===============================
#+end_example

* Using Options

#+BEGIN_SRC bash
pytest --help
#+END_SRC

** -collect-only

This shows which tests will be run with the given options and configurations

#+BEGIN_SRC bash
pytest --collect-only
#+END_SRC

** -k Expression

The -k option lets you use an expression to find what test functions to run.
It can be used as a shortcut to running an individual test if its name is unique,
or running a set of tests that have a common prefix or suffix in their names.

#+BEGIN_SRC bash
pytest -k "asdict or defaults" --collect-only
#+END_SRC

** -m MARKEXPR

Markers allow you to mark a subset of your test functions so that they can be run together.

Also accepts group of marks, like "mark1 and mark2", "mark1 and not mark2"

#+BEGIN_SRC python
import pytest

# ...

@pytest.mark.run_these_please
def test_member_access():
    pass
#+END_SRC

#+BEGIN_SRC bash
pytest -v -m run_these_please
#+END_SRC

** -x, -exitfirst

If a test fn encounters a failing assert or an exception, the execution for that test stops there and the test fails, then pytest runs the next test.
Sometimes stopping the entire test session immediately when a test fails is what we want though, and that's what the -x option does.

#+BEGIN_SRC bash
pytest -x
#+END_SRC

We can also turn off the stacktrace

#+BEGIN_SRC bash
pytest --tb=no
#+END_SRC

** -maxfail=num

This lets us specify how many failures are ok

#+BEGIN_SRC bash
pytest --maxfail=2 --tb=no
#+END_SRC

** -s and -capture=method

The -s flag allows print statements (or any stdout statement) to actually be printed to stdout when the tests are running.
It is a shortcut for -capture=no. This can help us do things like using a series of print() statements in our tests and
outputting the results to watch the flow of the test.

** -lf, -last-failed

This runs the last test that fails.

#+BEGIN_SRC bash
pytest --lf
#+END_SRC

** -ff, -failed-first

Does the same as --lf, and then runs the rest of the tests that passed as well.

#+BEGIN_SRC bash
pytest --ff --tb=no
#+END_SRC

** -v, -verbose

#+BEGIN_SRC bash
pytest -v -ff --tb=no
#+END_SRC

alternatively, we can quiet the output with -q, -quiet

** -l, -showlocals

This makes local variables and their values display with tracebacks for failing tests.

#+BEGIN_SRC bash
pytest -l tasks
#+END_SRC

** -tb=style

This modifies the way tracebacks for failures are output.
